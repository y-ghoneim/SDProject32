{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imblearn) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtendNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.26.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (3.8.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.4 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 13.2 MB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n"
     ]
    }
   ],
   "source": [
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from imblearn.under_sampling import ClusterCentroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model (using Exhaustive Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sleep_df = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "agg_sleep_df = pd.concat([agg_sleep_df, additional_df], axis=0, ignore_index=True)\n",
    "# Shuffle the DataFrame\n",
    "agg_sleep_df = agg_sleep_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "agg_sleep_df = agg_sleep_df.dropna()\n",
    "\n",
    "X = agg_sleep_df.loc[:,~agg_sleep_df.columns.isin(['patient_id','window_start','agitation'])]\n",
    "y = agg_sleep_df['agitation']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 848/848"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_RR', 'TST', 'SE', 'snoring_counts'], dtype='object')\n",
      "0.08\n"
     ]
    }
   ],
   "source": [
    "efs = ExhaustiveFeatureSelector(RandomForestClassifier(),min_features = 4, max_features=10, scoring='f1')\n",
    "\n",
    "efs = efs.fit(X_train, y_train)\n",
    "\n",
    "selected_features = X_train.columns[list(efs.best_idx_)] \n",
    "print(selected_features)\n",
    "\n",
    "print(efs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92       181\n",
      "         1.0       0.04      0.25      0.07         4\n",
      "\n",
      "    accuracy                           0.86       185\n",
      "   macro avg       0.51      0.56      0.50       185\n",
      "weighted avg       0.96      0.86      0.91       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with the best features on the resampled training set\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resampled[selected_features], y_train_resampled)\n",
    "# Ensure the test set is also limited to the selected features\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Predict the responses for the test set\n",
    "y_pred = classifier.predict(X_test_selected)\n",
    "# Generating the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Building a custom train-test split for our use-case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "data['source'] = 'real'  # Add a source column for original data\n",
    "\n",
    "# Load the synthetic data\n",
    "synthetic_data = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "synthetic_data['source'] = 'synthetic'  # Add a source column for synthetic data\n",
    "\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "data = pd.concat([data, synthetic_data], axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Splitting the data based on the 'agitation' column\n",
    "positive_data = data[data['agitation'] == 1]\n",
    "negative_data = data[data['agitation'] == 0]\n",
    "\n",
    "# Selecting test data from the original file for positive cases\n",
    "test_pos = positive_data[positive_data['source'] == 'real'].tail(14)  # Last 17 from original data for testing (near 20% split)\n",
    "# Remaining positive data from the real source (excluding those chosen for testing)\n",
    "train_pos_real = positive_data[positive_data['source'] == 'real'].drop(test_pos.index)\n",
    "\n",
    "# Selecting positive synthetic data for training\n",
    "train_pos_synthetic = positive_data[positive_data['source'] == 'synthetic']\n",
    "\n",
    "# Combine real and synthetic positive data for training\n",
    "train_pos_combined = pd.concat([train_pos_real, train_pos_synthetic], ignore_index=True)\n",
    "\n",
    "# Oversampling positive samples in the training set by 2x\n",
    "train_pos_oversampled = pd.concat([train_pos_combined] * 2, ignore_index=True)\n",
    "\n",
    "# Splitting negative data, random selection for the test set\n",
    "train_neg, test_neg = train_test_split(negative_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Matching the number of oversampled positive samples with negative samples in a 1:3 ratio\n",
    "train_neg_matched = train_neg.sample(n=len(train_pos_oversampled)*3, random_state=42)\n",
    "\n",
    "# Combining the matched training sets\n",
    "X_train = pd.concat([train_pos_oversampled, train_neg_matched])\n",
    "y_train = X_train['agitation']\n",
    "X_train = X_train.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Preparing the test set (stratify sampling manually to preserve the 1:3 ratio)\n",
    "test_neg_selected = test_neg.sample(n=80, random_state=42)  # Adjust n for desired test size\n",
    "X_test = pd.concat([test_pos, test_neg_selected])\n",
    "y_test = X_test['agitation']\n",
    "X_test = X_test.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_neg_matched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_neg_matched\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_neg_matched' is not defined"
     ]
    }
   ],
   "source": [
    "train_neg_matched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying undersampling and oversampling smartly to improve results (desperate attempt to improve results) - run either only this or run the codeblock above. They meet the same objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (805) found smaller than n_clusters (900). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "data = pd.read_csv('../Dataset/Aggregated_Sleep.csv')\n",
    "data['source'] = 'real'\n",
    "\n",
    "synthetic_data = pd.read_csv('../Dataset/Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "synthetic_data['source'] = 'synthetic'\n",
    "\n",
    "data = pd.concat([data, synthetic_data], ignore_index=True)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data based on 'agitation'\n",
    "positive_data = data[data['agitation'] == 1]\n",
    "negative_data = data[data['agitation'] == 0]\n",
    "\n",
    "# Test data selection\n",
    "test_pos = positive_data[positive_data['source'] == 'real'].tail(5)\n",
    "train_pos_real = positive_data[positive_data['source'] == 'real'].drop(test_pos.index)\n",
    "# Oversampling positive samples in the training set by 2x\n",
    "train_pos_oversampled = pd.concat([train_pos_real] * 2, ignore_index=True)\n",
    "train_pos_synthetic = positive_data[positive_data['source'] == 'synthetic']\n",
    "train_pos_combined = pd.concat([train_pos_oversampled, train_pos_synthetic], ignore_index=True)\n",
    "\n",
    "# Combine positive and negative data\n",
    "combined_data = pd.concat([train_pos_combined, negative_data], ignore_index=True)\n",
    "\n",
    "# Apply SMOTE to the entire dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(combined_data.drop(['agitation', 'source'], axis=1), combined_data['agitation'])\n",
    "\n",
    "# Apply Cluster Centroids\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_train_final, y_train_final = cc.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Separate the test data\n",
    "test_neg_selected = negative_data.sample(n=len(test_pos), random_state=42)\n",
    "X_test = pd.concat([test_pos, test_neg_selected])\n",
    "y_test = X_test['agitation']\n",
    "X_test = X_test.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest, One-Class SVM, Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      1.00      0.77         5\n",
      "         1.0       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.81      0.70      0.67        10\n",
      "weighted avg       0.81      0.70      0.67        10\n",
      "\n",
      "One-Class SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.60      0.60         5\n",
      "         1.0       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.60      0.60      0.60        10\n",
      "weighted avg       0.60      0.60      0.60        10\n",
      "\n",
      "Isolation Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.60      0.55         5\n",
      "         1.0       0.50      0.40      0.44         5\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.50      0.50      0.49        10\n",
      "weighted avg       0.50      0.50      0.49        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train_final)\n",
    "rf_predictions = rf_classifier.predict(X_test_scaled)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# One-Class SVM\n",
    "one_class_svm = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "one_class_svm.fit(X_train_scaled[y_train_final == 0])  # Train only on normal data for typical usage\n",
    "svm_predictions = one_class_svm.predict(X_test_scaled)\n",
    "svm_predictions = (svm_predictions == -1).astype(int)\n",
    "print(\"One-Class SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=300, contamination=float(np.mean(y_train_final == 1)), random_state=42)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "if_predictions = iso_forest.predict(X_test_scaled)\n",
    "if_predictions = (if_predictions == -1).astype(int)\n",
    "print(\"Isolation Forest Classification Report:\")\n",
    "print(classification_report(y_test, if_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (805) found smaller than n_clusters (900). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\ensemble\\_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      1.00      0.77         5\n",
      "         1.0       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.81      0.70      0.67        10\n",
      "weighted avg       0.81      0.70      0.67        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('../Dataset/Aggregated_Sleep.csv')\n",
    "data['source'] = 'real'\n",
    "synthetic_data = pd.read_csv('../Dataset/Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "synthetic_data['source'] = 'synthetic'\n",
    "\n",
    "# Combine real and synthetic datasets\n",
    "combined_data = pd.concat([data, synthetic_data], ignore_index=True)\n",
    "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "combined_data = combined_data.drop(['patient_id', 'window_start'], axis=1)\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Split data based on 'agitation'\n",
    "positive_data = combined_data[combined_data['agitation'] == 1]\n",
    "negative_data = combined_data[combined_data['agitation'] == 0]\n",
    "\n",
    "# Test data selection\n",
    "test_pos = positive_data[positive_data['source'] == 'real'].tail(5)\n",
    "train_pos_real = positive_data[positive_data['source'] == 'real'].drop(test_pos.index)\n",
    "# Oversampling positive samples in the training set by 2x\n",
    "train_pos_oversampled = pd.concat([train_pos_real] * 2, ignore_index=True)\n",
    "train_pos_synthetic = positive_data[positive_data['source'] == 'synthetic']\n",
    "train_pos_combined = pd.concat([train_pos_oversampled, train_pos_synthetic], ignore_index=True)\n",
    "\n",
    "# Combine positive and negative data for training\n",
    "train_data = pd.concat([train_pos_combined, negative_data], ignore_index=True)\n",
    "\n",
    "# Apply SMOTE to the entire dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(train_data.drop(['agitation', 'source'], axis=1), train_data['agitation'])\n",
    "\n",
    "# Apply Cluster Centroids\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "X_train_final, y_train_final = cc.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Separate the test data\n",
    "test_neg_selected = negative_data.sample(n=len(test_pos), random_state=42)\n",
    "X_test = pd.concat([test_pos, test_neg_selected])\n",
    "y_test = X_test['agitation']\n",
    "X_test = X_test.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Applying Balanced Random Forest Classifier\n",
    "brf_classifier = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "brf_classifier.fit(X_train_scaled, y_train_final)\n",
    "brf_predictions = brf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Classification report\n",
    "print(\"Balanced Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, brf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RUSBoostClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the RUSBoost model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m rusboost \u001b[38;5;241m=\u001b[39m RUSBoostClassifier(base_estimator\u001b[38;5;241m=\u001b[39m\u001b[43mDecisionTreeClassifier\u001b[49m(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m), \n\u001b[0;32m      5\u001b[0m                               n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m rusboost\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train_final)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "# Define the RUSBoost model\n",
    "rusboost = RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), \n",
    "                              n_estimators=200, random_state=42, algorithm='SAMME.R')\n",
    "\n",
    "# Train the model\n",
    "rusboost.fit(X_train_scaled, y_train_final)\n",
    "\n",
    "# Predict on the test set\n",
    "rus_predictions = rusboost.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, rus_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders (for anomaly detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6491299867630005\n",
      "Epoch 2, Loss: 0.20765209197998047\n",
      "Epoch 3, Loss: 0.6450648307800293\n",
      "Epoch 4, Loss: 0.6543410420417786\n",
      "Epoch 5, Loss: 0.6206033825874329\n",
      "Epoch 6, Loss: 0.28575924038887024\n",
      "Epoch 7, Loss: 0.21863439679145813\n",
      "Epoch 8, Loss: 0.19015732407569885\n",
      "Epoch 9, Loss: 0.4521961808204651\n",
      "Epoch 10, Loss: 0.28533846139907837\n",
      "Epoch 11, Loss: 0.8459001779556274\n",
      "Epoch 12, Loss: 0.22271554172039032\n",
      "Epoch 13, Loss: 0.4966084063053131\n",
      "Epoch 14, Loss: 0.11925645172595978\n",
      "Epoch 15, Loss: 0.6627262234687805\n",
      "Epoch 16, Loss: 0.08332616090774536\n",
      "Epoch 17, Loss: 0.11782519519329071\n",
      "Epoch 18, Loss: 0.47434431314468384\n",
      "Epoch 19, Loss: 0.24924464523792267\n",
      "Epoch 20, Loss: 0.19413729012012482\n",
      "Autoencoder Anomaly Detection Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.93      0.65        14\n",
      "         1.0       0.50      0.07      0.12        14\n",
      "\n",
      "    accuracy                           0.50        28\n",
      "   macro avg       0.50      0.50      0.39        28\n",
      "weighted avg       0.50      0.50      0.39        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Autoencoder Architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_train_final.shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, X_train_final.shape[1]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# 2. Prepare Data Loaders\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float())\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. Initialize the Autoencoder and Optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Train the Autoencoder\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 5. Evaluate the Model for Anomaly Detection\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test_scaled).float()\n",
    "    reconstructed = model(X_test_tensor)\n",
    "    mse = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1)\n",
    "    anomaly_threshold = np.percentile(mse.numpy(), 95)  # Adjust based on your preference\n",
    "    test_predictions = (mse > anomaly_threshold).int().numpy()\n",
    "\n",
    "# 6. Print Classification Report\n",
    "print(\"Autoencoder Anomaly Detection Report:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Neural Network (with weighted loss) - Best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the custom train-test codeblock before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6250805813690712\n",
      "Epoch 2/50, Loss: 0.46620563083681565\n",
      "Epoch 3/50, Loss: 0.35731173126861965\n",
      "Epoch 4/50, Loss: 0.28349960518294365\n",
      "Epoch 5/50, Loss: 0.23819801416890374\n",
      "Epoch 6/50, Loss: 0.20993695999013967\n",
      "Epoch 7/50, Loss: 0.17821118389738017\n",
      "Epoch 8/50, Loss: 0.15856846344882045\n",
      "Epoch 9/50, Loss: 0.14343754664577288\n",
      "Epoch 10/50, Loss: 0.14080321197879725\n",
      "Epoch 11/50, Loss: 0.1175454106695693\n",
      "Epoch 12/50, Loss: 0.10982097415574665\n",
      "Epoch 13/50, Loss: 0.10075320043312065\n",
      "Epoch 14/50, Loss: 0.09738036938782396\n",
      "Epoch 15/50, Loss: 0.09474801079466424\n",
      "Epoch 16/50, Loss: 0.07988212887069275\n",
      "Epoch 17/50, Loss: 0.07451666686041601\n",
      "Epoch 18/50, Loss: 0.06803612475251329\n",
      "Epoch 19/50, Loss: 0.06478351071990769\n",
      "Epoch 20/50, Loss: 0.05656262984948939\n",
      "Epoch 21/50, Loss: 0.05420024696223695\n",
      "Epoch 22/50, Loss: 0.05737873378755717\n",
      "Epoch 23/50, Loss: 0.04595558533990948\n",
      "Epoch 24/50, Loss: 0.05074834282477868\n",
      "Epoch 25/50, Loss: 0.03882034405552108\n",
      "Epoch 26/50, Loss: 0.03993152772429688\n",
      "Epoch 27/50, Loss: 0.041947255392783676\n",
      "Epoch 28/50, Loss: 0.03395607649220218\n",
      "Epoch 29/50, Loss: 0.03287877916390526\n",
      "Epoch 30/50, Loss: 0.027281640828105396\n",
      "Epoch 31/50, Loss: 0.028092243805013854\n",
      "Epoch 32/50, Loss: 0.03239340339947877\n",
      "Epoch 33/50, Loss: 0.024853508501988034\n",
      "Epoch 34/50, Loss: 0.02184678586842171\n",
      "Epoch 35/50, Loss: 0.017371027874920904\n",
      "Epoch 36/50, Loss: 0.016797814800219354\n",
      "Epoch 37/50, Loss: 0.015668866842792464\n",
      "Epoch 38/50, Loss: 0.01384458793648358\n",
      "Epoch 39/50, Loss: 0.013444798669360322\n",
      "Epoch 40/50, Loss: 0.012278514246175072\n",
      "Epoch 41/50, Loss: 0.011199930899551716\n",
      "Epoch 42/50, Loss: 0.01734217440966388\n",
      "Epoch 43/50, Loss: 0.011839829212664786\n",
      "Epoch 44/50, Loss: 0.008620557628154498\n",
      "Epoch 45/50, Loss: 0.007391512550509952\n",
      "Epoch 46/50, Loss: 0.008249492334314334\n",
      "Epoch 47/50, Loss: 0.006499930846922356\n",
      "Epoch 48/50, Loss: 0.0060105422423381745\n",
      "Epoch 49/50, Loss: 0.005480519311245659\n",
      "Epoch 50/50, Loss: 0.007415650743621819\n",
      "Detailed Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.71         5\n",
      "           1       1.00      0.20      0.33         5\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.78      0.60      0.52        10\n",
      "weighted avg       0.78      0.60      0.52        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_counts = y_train_final.value_counts()\n",
    "total_samples = len(y_train_final)\n",
    "weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = torch.tensor(weights).float()\n",
    "\n",
    "# Define the Neural Network Architecture\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers):\n",
    "        super(TabularModel, self).__init__()\n",
    "        layers = []\n",
    "        for i in hidden_layers:\n",
    "            layers.append(nn.Linear(num_inputs, i))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            num_inputs = i\n",
    "        layers.append(nn.Linear(num_inputs, num_outputs))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the model, criterion (now weighted), and optimizer\n",
    "model = TabularModel(X_train_scaled.shape[1], 2, [50, 100, 50])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float(), torch.tensor(y_train_final.values).long())\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the Network\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Prepare DataLoader for test data\n",
    "test_data = TensorDataset(torch.tensor(X_test_scaled).float(), torch.tensor(y_test.values).long())\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds_test.extend(predicted.numpy())\n",
    "        all_targets_test.extend(targets.numpy())\n",
    "\n",
    "print(\"Detailed Classification Report on Test Data:\")\n",
    "print(classification_report(all_targets_test, all_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Neural Network just with stratified sampling (no custom split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.6857850689154404\n",
      "Epoch 2/40, Loss: 0.6335163070605352\n",
      "Epoch 3/40, Loss: 0.5792205104461083\n",
      "Epoch 4/40, Loss: 0.5248727271190057\n",
      "Epoch 5/40, Loss: 0.4788503371752225\n",
      "Epoch 6/40, Loss: 0.4484001306387094\n",
      "Epoch 7/40, Loss: 0.42704424949792713\n",
      "Epoch 8/40, Loss: 0.4164835696036999\n",
      "Epoch 9/40, Loss: 0.35882395047407883\n",
      "Epoch 10/40, Loss: 0.3613038338147677\n",
      "Epoch 11/40, Loss: 0.335318556198707\n",
      "Epoch 12/40, Loss: 0.320053654221388\n",
      "Epoch 13/40, Loss: 0.32075402255241686\n",
      "Epoch 14/40, Loss: 0.302602456166194\n",
      "Epoch 15/40, Loss: 0.31414736348849076\n",
      "Epoch 16/40, Loss: 0.27527306916622013\n",
      "Epoch 17/40, Loss: 0.28774605748745113\n",
      "Epoch 18/40, Loss: 0.2809354020999028\n",
      "Epoch 19/40, Loss: 0.23380409653943318\n",
      "Epoch 20/40, Loss: 0.21943960718523997\n",
      "Epoch 21/40, Loss: 0.21910991233128768\n",
      "Epoch 22/40, Loss: 0.21519857559066552\n",
      "Epoch 23/40, Loss: 0.20325964460006127\n",
      "Epoch 24/40, Loss: 0.20063808732307875\n",
      "Epoch 25/40, Loss: 0.1979191039617245\n",
      "Epoch 26/40, Loss: 0.17425015597389296\n",
      "Epoch 27/40, Loss: 0.20014269821918929\n",
      "Epoch 28/40, Loss: 0.17494646230569252\n",
      "Epoch 29/40, Loss: 0.17312458272163683\n",
      "Epoch 30/40, Loss: 0.14928602140683395\n",
      "Epoch 31/40, Loss: 0.1513436367878547\n",
      "Epoch 32/40, Loss: 0.1399510780779215\n",
      "Epoch 33/40, Loss: 0.1397472178706756\n",
      "Epoch 34/40, Loss: 0.1307386331833326\n",
      "Epoch 35/40, Loss: 0.12015102588786529\n",
      "Epoch 36/40, Loss: 0.11230499841845952\n",
      "Epoch 37/40, Loss: 0.11405206758242387\n",
      "Epoch 38/40, Loss: 0.13355203316761896\n",
      "Epoch 39/40, Loss: 0.1186075176183994\n",
      "Epoch 40/40, Loss: 0.10494450116171859\n",
      "Detailed Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       181\n",
      "           1       0.36      0.69      0.47        13\n",
      "\n",
      "    accuracy                           0.90       194\n",
      "   macro avg       0.67      0.80      0.71       194\n",
      "weighted avg       0.94      0.90      0.91       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "data = pd.concat([data, additional_df], axis=0, ignore_index=True)\n",
    "# Shuffle the DataFrame\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Stratify split the data into training and testing sets\n",
    "X = data.drop('agitation', axis=1)\n",
    "y = data['agitation']\n",
    "\n",
    "# Stratify sample based on the class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = y_train.value_counts()\n",
    "total_samples = len(y_train)\n",
    "weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = torch.tensor(weights).float()\n",
    "\n",
    "#From this point on, the code is the same as the previous example\n",
    "\n",
    "# Define the Neural Network Architecture\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers):\n",
    "        super(TabularModel, self).__init__()\n",
    "        layers = []\n",
    "        for i in hidden_layers:\n",
    "            layers.append(nn.Linear(num_inputs, i))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            num_inputs = i\n",
    "        layers.append(nn.Linear(num_inputs, num_outputs))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the model, criterion (now weighted), and optimizer\n",
    "model = TabularModel(X_train_scaled.shape[1], 2, [50, 100, 50])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float(), torch.tensor(y_train.values).long())\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the Network\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Prepare DataLoader for test data\n",
    "test_data = TensorDataset(torch.tensor(X_test_scaled).float(), torch.tensor(y_test.values).long())\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds_test.extend(predicted.numpy())\n",
    "        all_targets_test.extend(targets.numpy())\n",
    "\n",
    "print(\"Detailed Classification Report on Test Data:\")\n",
    "print(classification_report(all_targets_test, all_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results compare to our old best model because we added the synthetic data to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder + One Class SVM Ensemble Model (needs testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.1442\n",
      "Epoch 11/50, Loss: 0.5622\n",
      "Epoch 21/50, Loss: 0.4725\n",
      "Epoch 31/50, Loss: 0.8408\n",
      "Epoch 41/50, Loss: 0.7065\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.04      1.00      0.08         7\n",
      "        True       1.00      0.15      0.25       178\n",
      "\n",
      "    accuracy                           0.18       185\n",
      "   macro avg       0.52      0.57      0.17       185\n",
      "weighted avg       0.96      0.18      0.25       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Dataset/Aggregated_Sleep.csv')\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "\n",
    "# Drop rows with NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "X = data.drop('agitation', axis=1).values\n",
    "y = data['agitation'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data, focusing only on normal data for training the Autoencoder\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y == 0, test_size=0.2, random_state=42)\n",
    "X_train_normal = X_train[y_train]\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Training the Autoencoder\n",
    "autoencoder = Autoencoder(X_train_normal.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_normal).float()), batch_size=32, shuffle=True)\n",
    "\n",
    "def train_autoencoder(model, loader, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data in loader:\n",
    "            data = data[0]  # unpack data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "train_autoencoder(autoencoder, train_loader)\n",
    "\n",
    "# Extract features for training and testing\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_encoded = autoencoder.encoder(torch.tensor(X_train).float()).numpy()\n",
    "    X_test_encoded = autoencoder.encoder(torch.tensor(X_test).float()).numpy()\n",
    "\n",
    "# Train One-Class SVM on the encoded features\n",
    "oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "oc_svm.fit(X_train_encoded[y_train])  # Train only on normal data\n",
    "\n",
    "y_pred_train = oc_svm.predict(X_train_encoded)\n",
    "y_pred_test = oc_svm.predict(X_test_encoded)\n",
    "y_pred_test = np.where(y_pred_test == 1, 0, 1)  # Converting from SVM labels to anomaly labels\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
