{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imblearn) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtendNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.26.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (3.8.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.4 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 13.2 MB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n"
     ]
    }
   ],
   "source": [
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model (using Exhaustive Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sleep_df = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "agg_sleep_df = pd.concat([agg_sleep_df, additional_df], axis=0, ignore_index=True)\n",
    "# Shuffle the DataFrame\n",
    "agg_sleep_df = agg_sleep_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "agg_sleep_df = agg_sleep_df.dropna()\n",
    "\n",
    "X = agg_sleep_df.loc[:,~agg_sleep_df.columns.isin(['patient_id','window_start','agitation'])]\n",
    "y = agg_sleep_df['agitation']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features: 848/848"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_RR', 'TST', 'SE', 'snoring_counts'], dtype='object')\n",
      "0.08\n"
     ]
    }
   ],
   "source": [
    "efs = ExhaustiveFeatureSelector(RandomForestClassifier(),min_features = 4, max_features=10, scoring='f1')\n",
    "\n",
    "efs = efs.fit(X_train, y_train)\n",
    "\n",
    "selected_features = X_train.columns[list(efs.best_idx_)] \n",
    "print(selected_features)\n",
    "\n",
    "print(efs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92       181\n",
      "         1.0       0.04      0.25      0.07         4\n",
      "\n",
      "    accuracy                           0.86       185\n",
      "   macro avg       0.51      0.56      0.50       185\n",
      "weighted avg       0.96      0.86      0.91       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with the best features on the resampled training set\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resampled[selected_features], y_train_resampled)\n",
    "# Ensure the test set is also limited to the selected features\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Predict the responses for the test set\n",
    "y_pred = classifier.predict(X_test_selected)\n",
    "# Generating the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Building a custom train-test split for our use-case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "data['source'] = 'real'  # Add a source column for original data\n",
    "\n",
    "# Load the synthetic data\n",
    "synthetic_data = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "synthetic_data['source'] = 'synthetic'  # Add a source column for synthetic data\n",
    "\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "data = pd.concat([data, synthetic_data], axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Splitting the data based on the 'agitation' column\n",
    "positive_data = data[data['agitation'] == 1]\n",
    "negative_data = data[data['agitation'] == 0]\n",
    "\n",
    "# Selecting test data from the original file for positive cases\n",
    "test_pos = positive_data[positive_data['source'] == 'real'].tail(13)  # Last 17 from original data for testing (near 20% split)\n",
    "# Remaining positive data from the real source (excluding those chosen for testing)\n",
    "train_pos_real = positive_data[positive_data['source'] == 'real'].drop(test_pos.index)\n",
    "\n",
    "# Selecting positive synthetic data for training\n",
    "train_pos_synthetic = positive_data[positive_data['source'] == 'synthetic']\n",
    "\n",
    "# Combine real and synthetic positive data for training\n",
    "train_pos_combined = pd.concat([train_pos_real, train_pos_synthetic], ignore_index=True)\n",
    "\n",
    "# Oversampling positive samples in the training set by 2x\n",
    "train_pos_oversampled = pd.concat([train_pos_combined] * 2, ignore_index=True)\n",
    "\n",
    "# Splitting negative data, random selection for the test set\n",
    "train_neg, test_neg = train_test_split(negative_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Matching the number of oversampled positive samples with negative samples in a 1:3 ratio\n",
    "train_neg_matched = train_neg.sample(n=len(train_pos_oversampled)*3, random_state=42)\n",
    "\n",
    "# Combining the matched training sets\n",
    "X_train = pd.concat([train_pos_oversampled, train_neg_matched])\n",
    "y_train = X_train['agitation']\n",
    "X_train = X_train.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Preparing the test set (stratify sampling manually to preserve the 1:3 ratio)\n",
    "test_neg_selected = test_neg.sample(n=80, random_state=42)  # Adjust n for desired test size\n",
    "X_test = pd.concat([test_pos, test_neg_selected])\n",
    "y_test = X_test['agitation']\n",
    "X_test = X_test.drop(['agitation', 'source'], axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 12)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324, 12)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_matched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 12)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_neg_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest, One-Class SVM, Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.99      0.92        80\n",
      "         1.0       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.85        93\n",
      "   macro avg       0.43      0.49      0.46        93\n",
      "weighted avg       0.74      0.85      0.79        93\n",
      "\n",
      "One-Class SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.49      0.61        80\n",
      "         1.0       0.11      0.38      0.17        13\n",
      "\n",
      "    accuracy                           0.47        93\n",
      "   macro avg       0.47      0.44      0.39        93\n",
      "weighted avg       0.73      0.47      0.55        93\n",
      "\n",
      "Isolation Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.70      0.77        80\n",
      "         1.0       0.11      0.23      0.15        13\n",
      "\n",
      "    accuracy                           0.63        93\n",
      "   macro avg       0.48      0.47      0.46        93\n",
      "weighted avg       0.75      0.63      0.68        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test_scaled)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# One-Class SVM\n",
    "one_class_svm = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "one_class_svm.fit(X_train_scaled[y_train == 0])  # Train only on normal data for typical usage\n",
    "svm_predictions = one_class_svm.predict(X_test_scaled)\n",
    "svm_predictions = (svm_predictions == -1).astype(int)\n",
    "print(\"One-Class SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=200, contamination=float(np.mean(y_train == 1)), random_state=42)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "if_predictions = iso_forest.predict(X_test_scaled)\n",
    "if_predictions = (if_predictions == -1).astype(int)\n",
    "print(\"Isolation Forest Classification Report:\")\n",
    "print(classification_report(y_test, if_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders (for anomaly detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4859554171562195\n",
      "Epoch 2, Loss: 1.5452849864959717\n",
      "Epoch 3, Loss: 0.9651633501052856\n",
      "Epoch 4, Loss: 0.7909493446350098\n",
      "Epoch 5, Loss: 0.4672658443450928\n",
      "Epoch 6, Loss: 0.6086257696151733\n",
      "Epoch 7, Loss: 0.9460229873657227\n",
      "Epoch 8, Loss: 0.4436676502227783\n",
      "Epoch 9, Loss: 0.8346527814865112\n",
      "Epoch 10, Loss: 0.4320221543312073\n",
      "Epoch 11, Loss: 0.6609364748001099\n",
      "Epoch 12, Loss: 0.6512584090232849\n",
      "Epoch 13, Loss: 0.4657561779022217\n",
      "Epoch 14, Loss: 0.25464311242103577\n",
      "Epoch 15, Loss: 0.37018486857414246\n",
      "Epoch 16, Loss: 0.36595091223716736\n",
      "Epoch 17, Loss: 0.30109405517578125\n",
      "Epoch 18, Loss: 0.24488189816474915\n",
      "Epoch 19, Loss: 0.3811655640602112\n",
      "Epoch 20, Loss: 0.44289645552635193\n",
      "Autoencoder Anomaly Detection Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90        80\n",
      "         1.0       0.20      0.08      0.11        13\n",
      "\n",
      "    accuracy                           0.83        93\n",
      "   macro avg       0.53      0.51      0.51        93\n",
      "weighted avg       0.77      0.83      0.79        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Autoencoder Architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, X_train.shape[1]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# 2. Prepare Data Loaders\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float())\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. Initialize the Autoencoder and Optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Train the Autoencoder\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 5. Evaluate the Model for Anomaly Detection\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test_scaled).float()\n",
    "    reconstructed = model(X_test_tensor)\n",
    "    mse = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1)\n",
    "    anomaly_threshold = np.percentile(mse.numpy(), 95)  # Adjust based on your preference\n",
    "    test_predictions = (mse > anomaly_threshold).int().numpy()\n",
    "\n",
    "# 6. Print Classification Report\n",
    "print(\"Autoencoder Anomaly Detection Report:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Neural Network (with weighted loss) - Best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the custom train-test codeblock before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6799209884234837\n",
      "Epoch 2/50, Loss: 0.6471615944589887\n",
      "Epoch 3/50, Loss: 0.6037799971444267\n",
      "Epoch 4/50, Loss: 0.5457319617271423\n",
      "Epoch 5/50, Loss: 0.4770395074571882\n",
      "Epoch 6/50, Loss: 0.41332034128052847\n",
      "Epoch 7/50, Loss: 0.3485840473856245\n",
      "Epoch 8/50, Loss: 0.3104704873902457\n",
      "Epoch 9/50, Loss: 0.29070272190230234\n",
      "Epoch 10/50, Loss: 0.2654046565294266\n",
      "Epoch 11/50, Loss: 0.2519577273300716\n",
      "Epoch 12/50, Loss: 0.23400978105408804\n",
      "Epoch 13/50, Loss: 0.22520380999360765\n",
      "Epoch 14/50, Loss: 0.2166804756437029\n",
      "Epoch 15/50, Loss: 0.2088304257818631\n",
      "Epoch 16/50, Loss: 0.19534272168363845\n",
      "Epoch 17/50, Loss: 0.18324861249753407\n",
      "Epoch 18/50, Loss: 0.1677611353141921\n",
      "Epoch 19/50, Loss: 0.15242711773940495\n",
      "Epoch 20/50, Loss: 0.14549020039183752\n",
      "Epoch 21/50, Loss: 0.13257803235735213\n",
      "Epoch 22/50, Loss: 0.12231102266481944\n",
      "Epoch 23/50, Loss: 0.10988135210105351\n",
      "Epoch 24/50, Loss: 0.10155476682952472\n",
      "Epoch 25/50, Loss: 0.09116095091615405\n",
      "Epoch 26/50, Loss: 0.08519478035824639\n",
      "Epoch 27/50, Loss: 0.07686036505869456\n",
      "Epoch 28/50, Loss: 0.07577040046453476\n",
      "Epoch 29/50, Loss: 0.06861849980694908\n",
      "Epoch 30/50, Loss: 0.06443626806139946\n",
      "Epoch 31/50, Loss: 0.05502778611012867\n",
      "Epoch 32/50, Loss: 0.05135924688407353\n",
      "Epoch 33/50, Loss: 0.04776129738560745\n",
      "Epoch 34/50, Loss: 0.04936219698616436\n",
      "Epoch 35/50, Loss: 0.04988387519759791\n",
      "Epoch 36/50, Loss: 0.0378456392458507\n",
      "Epoch 37/50, Loss: 0.037677964993885586\n",
      "Epoch 38/50, Loss: 0.034852850650038035\n",
      "Epoch 39/50, Loss: 0.03412641451827118\n",
      "Epoch 40/50, Loss: 0.03226903293813978\n",
      "Epoch 41/50, Loss: 0.02800338835056339\n",
      "Epoch 42/50, Loss: 0.02433274021106107\n",
      "Epoch 43/50, Loss: 0.02558250099952732\n",
      "Epoch 44/50, Loss: 0.024361940899065564\n",
      "Epoch 45/50, Loss: 0.02516653042818819\n",
      "Epoch 46/50, Loss: 0.023807433300784657\n",
      "Epoch 47/50, Loss: 0.019792555831372738\n",
      "Epoch 48/50, Loss: 0.02001633688009211\n",
      "Epoch 49/50, Loss: 0.017659072697694813\n",
      "Epoch 50/50, Loss: 0.016527664049395492\n",
      "Detailed Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92        80\n",
      "           1       0.50      0.23      0.32        13\n",
      "\n",
      "    accuracy                           0.86        93\n",
      "   macro avg       0.69      0.60      0.62        93\n",
      "weighted avg       0.83      0.86      0.84        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_counts = y_train.value_counts()\n",
    "total_samples = len(y_train)\n",
    "weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = torch.tensor(weights).float()\n",
    "\n",
    "# Define the Neural Network Architecture\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers):\n",
    "        super(TabularModel, self).__init__()\n",
    "        layers = []\n",
    "        for i in hidden_layers:\n",
    "            layers.append(nn.Linear(num_inputs, i))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            num_inputs = i\n",
    "        layers.append(nn.Linear(num_inputs, num_outputs))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the model, criterion (now weighted), and optimizer\n",
    "model = TabularModel(X_train_scaled.shape[1], 2, [50, 100, 50])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float(), torch.tensor(y_train.values).long())\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the Network\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Prepare DataLoader for test data\n",
    "test_data = TensorDataset(torch.tensor(X_test_scaled).float(), torch.tensor(y_test.values).long())\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds_test.extend(predicted.numpy())\n",
    "        all_targets_test.extend(targets.numpy())\n",
    "\n",
    "print(\"Detailed Classification Report on Test Data:\")\n",
    "print(classification_report(all_targets_test, all_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Neural Network just with stratified sampling (no custom split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.6857850689154404\n",
      "Epoch 2/40, Loss: 0.6335163070605352\n",
      "Epoch 3/40, Loss: 0.5792205104461083\n",
      "Epoch 4/40, Loss: 0.5248727271190057\n",
      "Epoch 5/40, Loss: 0.4788503371752225\n",
      "Epoch 6/40, Loss: 0.4484001306387094\n",
      "Epoch 7/40, Loss: 0.42704424949792713\n",
      "Epoch 8/40, Loss: 0.4164835696036999\n",
      "Epoch 9/40, Loss: 0.35882395047407883\n",
      "Epoch 10/40, Loss: 0.3613038338147677\n",
      "Epoch 11/40, Loss: 0.335318556198707\n",
      "Epoch 12/40, Loss: 0.320053654221388\n",
      "Epoch 13/40, Loss: 0.32075402255241686\n",
      "Epoch 14/40, Loss: 0.302602456166194\n",
      "Epoch 15/40, Loss: 0.31414736348849076\n",
      "Epoch 16/40, Loss: 0.27527306916622013\n",
      "Epoch 17/40, Loss: 0.28774605748745113\n",
      "Epoch 18/40, Loss: 0.2809354020999028\n",
      "Epoch 19/40, Loss: 0.23380409653943318\n",
      "Epoch 20/40, Loss: 0.21943960718523997\n",
      "Epoch 21/40, Loss: 0.21910991233128768\n",
      "Epoch 22/40, Loss: 0.21519857559066552\n",
      "Epoch 23/40, Loss: 0.20325964460006127\n",
      "Epoch 24/40, Loss: 0.20063808732307875\n",
      "Epoch 25/40, Loss: 0.1979191039617245\n",
      "Epoch 26/40, Loss: 0.17425015597389296\n",
      "Epoch 27/40, Loss: 0.20014269821918929\n",
      "Epoch 28/40, Loss: 0.17494646230569252\n",
      "Epoch 29/40, Loss: 0.17312458272163683\n",
      "Epoch 30/40, Loss: 0.14928602140683395\n",
      "Epoch 31/40, Loss: 0.1513436367878547\n",
      "Epoch 32/40, Loss: 0.1399510780779215\n",
      "Epoch 33/40, Loss: 0.1397472178706756\n",
      "Epoch 34/40, Loss: 0.1307386331833326\n",
      "Epoch 35/40, Loss: 0.12015102588786529\n",
      "Epoch 36/40, Loss: 0.11230499841845952\n",
      "Epoch 37/40, Loss: 0.11405206758242387\n",
      "Epoch 38/40, Loss: 0.13355203316761896\n",
      "Epoch 39/40, Loss: 0.1186075176183994\n",
      "Epoch 40/40, Loss: 0.10494450116171859\n",
      "Detailed Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       181\n",
      "           1       0.36      0.69      0.47        13\n",
      "\n",
      "    accuracy                           0.90       194\n",
      "   macro avg       0.67      0.80      0.71       194\n",
      "weighted avg       0.94      0.90      0.91       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the dataset\n",
    "data = pd.read_csv('..\\Dataset\\Aggregated_Sleep.csv')\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('..\\Dataset\\Synthetic_Sleep_Anomaly_CTGAN.csv')\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "data = pd.concat([data, additional_df], axis=0, ignore_index=True)\n",
    "# Shuffle the DataFrame\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Stratify split the data into training and testing sets\n",
    "X = data.drop('agitation', axis=1)\n",
    "y = data['agitation']\n",
    "\n",
    "# Stratify sample based on the class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = y_train.value_counts()\n",
    "total_samples = len(y_train)\n",
    "weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = torch.tensor(weights).float()\n",
    "\n",
    "#From this point on, the code is the same as the previous example\n",
    "\n",
    "# Define the Neural Network Architecture\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers):\n",
    "        super(TabularModel, self).__init__()\n",
    "        layers = []\n",
    "        for i in hidden_layers:\n",
    "            layers.append(nn.Linear(num_inputs, i))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            num_inputs = i\n",
    "        layers.append(nn.Linear(num_inputs, num_outputs))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the model, criterion (now weighted), and optimizer\n",
    "model = TabularModel(X_train_scaled.shape[1], 2, [50, 100, 50])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_data = TensorDataset(torch.tensor(X_train_scaled).float(), torch.tensor(y_train.values).long())\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the Network\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "# Prepare DataLoader for test data\n",
    "test_data = TensorDataset(torch.tensor(X_test_scaled).float(), torch.tensor(y_test.values).long())\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the Model on Test Data\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds_test.extend(predicted.numpy())\n",
    "        all_targets_test.extend(targets.numpy())\n",
    "\n",
    "print(\"Detailed Classification Report on Test Data:\")\n",
    "print(classification_report(all_targets_test, all_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results compare to our old best model because we added the synthetic data to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder + One Class SVM Ensemble Model (needs testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.1442\n",
      "Epoch 11/50, Loss: 0.5622\n",
      "Epoch 21/50, Loss: 0.4725\n",
      "Epoch 31/50, Loss: 0.8408\n",
      "Epoch 41/50, Loss: 0.7065\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.04      1.00      0.08         7\n",
      "        True       1.00      0.15      0.25       178\n",
      "\n",
      "    accuracy                           0.18       185\n",
      "   macro avg       0.52      0.57      0.17       185\n",
      "weighted avg       0.96      0.18      0.25       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Dataset/Aggregated_Sleep.csv')\n",
    "data = data.drop(['patient_id', 'window_start'], axis=1)\n",
    "\n",
    "# Drop rows with NaNs\n",
    "data = data.dropna()\n",
    "\n",
    "X = data.drop('agitation', axis=1).values\n",
    "y = data['agitation'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data, focusing only on normal data for training the Autoencoder\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y == 0, test_size=0.2, random_state=42)\n",
    "X_train_normal = X_train[y_train]\n",
    "\n",
    "# Define the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Training the Autoencoder\n",
    "autoencoder = Autoencoder(X_train_normal.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_normal).float()), batch_size=32, shuffle=True)\n",
    "\n",
    "def train_autoencoder(model, loader, epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data in loader:\n",
    "            data = data[0]  # unpack data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "train_autoencoder(autoencoder, train_loader)\n",
    "\n",
    "# Extract features for training and testing\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_encoded = autoencoder.encoder(torch.tensor(X_train).float()).numpy()\n",
    "    X_test_encoded = autoencoder.encoder(torch.tensor(X_test).float()).numpy()\n",
    "\n",
    "# Train One-Class SVM on the encoded features\n",
    "oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)\n",
    "oc_svm.fit(X_train_encoded[y_train])  # Train only on normal data\n",
    "\n",
    "y_pred_train = oc_svm.predict(X_train_encoded)\n",
    "y_pred_test = oc_svm.predict(X_test_encoded)\n",
    "y_pred_test = np.where(y_pred_test == 1, 0, 1)  # Converting from SVM labels to anomaly labels\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
